{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ETtu9CvVMDR"
      },
      "source": [
        "<h1>Chapter 7 - Advanced Text Generation Techniques and Tools</h1>\n",
        "<i>Going beyond prompt engineering.</i>\n",
        "\n",
        "This notebook is for Chapter 7 of the [Hands-On Large Language Models](https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961) book by [Jay Alammar](https://www.linkedin.com/in/jalammar) and [Maarten Grootendorst](https://www.linkedin.com/in/mgrootendorst/).\n",
        "\n",
        "This chapter explores using the LangChain framework to improve LLM performance with model I/O, helping LLMs remember, combining complex behavior, and chaining. The LangChain alternatives include DSPy and Haystack."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj_OKvyhG03L"
      },
      "source": [
        "### Install Packages on <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n",
        "\n",
        "To run in Google Colab (or any cloud vendor), **uncomment and run** the following codeblock to install the dependencies for this chapter:\n",
        "\n",
        "---\n",
        "\n",
        "ðŸ’¡ **NOTE**: Use a GPU to run the examples in this notebook. In Google Colab, go to\n",
        "**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DdD18CwdG03O"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain>=0.1.17 openai>=1.13.3 langchain_openai>=0.1.6 transformers>=4.40.1 datasets>=2.18.0 accelerate>=0.27.2 sentence-transformers>=2.5.1 duckduckgo-search>=5.2.2 langchain_community\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUDA=on\" pip install llama-cpp-python==0.2.69"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rerbJgwAigbK"
      },
      "source": [
        "# Loading Quantized Models with LangChain\n",
        "\n",
        "Quantization reduces the number of bits representing LLM parameters. The performance boost from reduced model weight can more than offset the loss in precision. This notebook uses a GGUF model variant of Phi-3. GGUF is a model format used in the llama.cpp library that incorporates quantization techniques to optimize model performance. The GGUF format of [Phi-3-Mini-4K-Instruct](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) is the quantized 8-bit variant of the 16-bit model.\n",
        "\n",
        "Prior chapters used the AutoModelForCausalLM class from the transformers library to load models from Hugging Face. To work with the quantized version of the model in LangChain, manually download the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2Z7b8QlpG03T",
        "outputId": "cb0ecefb-4077-4806-92eb-574ae2e4a2ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-14 14:05:54--  https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.165.160.59, 3.165.160.12, 3.165.160.11, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.165.160.59|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/41/c8/41c860f65b01de5dc4c68b00d84cead799d3e7c48e38ee749f4c6057776e2e9e/5d99003e395775659b0dde3f941d88ff378b2837a8dc3a2ea94222ab1420fad3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&Expires=1734444354&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNDQ0NDM1NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQxL2M4LzQxYzg2MGY2NWIwMWRlNWRjNGM2OGIwMGQ4NGNlYWQ3OTlkM2U3YzQ4ZTM4ZWU3NDlmNGM2MDU3Nzc2ZTJlOWUvNWQ5OTAwM2UzOTU3NzU2NTliMGRkZTNmOTQxZDg4ZmYzNzhiMjgzN2E4ZGMzYTJlYTk0MjIyYWIxNDIwZmFkMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=NPm1%7EYjiKElJAW0DljarecfKJ1Be4jGquuF8ykMEgheycdwbDHZ4UGLi3eRLA2w3bJC0Nb1dS12pcsUQ4sO0v3DkHf1c7MYcRvhuqPsw7vFS4vilQcp1zYBymQ7owiO2d7eg0bnCr0AQy-BanD9Co%7ELgC2Zfg0hMX7CIv7HmPjQ5Nb%7EXp-JRnptb2%7E1qRBEvTlm-wzvT3ruhI53PgbPHvC1ulULmIbuEINiLwaiSIW9WvjV6ZiXmBFjt2xuAAjKyjaDTqC-VRz0Fs1egLouN69%7EZIfgDDqORIlM5FIcCYZwrjI5XOCMTpH6QxoelB5ykI7HwbMqNTcmv35eveeMSZQ__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2024-12-14 14:05:54--  https://cdn-lfs-us-1.hf.co/repos/41/c8/41c860f65b01de5dc4c68b00d84cead799d3e7c48e38ee749f4c6057776e2e9e/5d99003e395775659b0dde3f941d88ff378b2837a8dc3a2ea94222ab1420fad3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&Expires=1734444354&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNDQ0NDM1NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQxL2M4LzQxYzg2MGY2NWIwMWRlNWRjNGM2OGIwMGQ4NGNlYWQ3OTlkM2U3YzQ4ZTM4ZWU3NDlmNGM2MDU3Nzc2ZTJlOWUvNWQ5OTAwM2UzOTU3NzU2NTliMGRkZTNmOTQxZDg4ZmYzNzhiMjgzN2E4ZGMzYTJlYTk0MjIyYWIxNDIwZmFkMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=NPm1%7EYjiKElJAW0DljarecfKJ1Be4jGquuF8ykMEgheycdwbDHZ4UGLi3eRLA2w3bJC0Nb1dS12pcsUQ4sO0v3DkHf1c7MYcRvhuqPsw7vFS4vilQcp1zYBymQ7owiO2d7eg0bnCr0AQy-BanD9Co%7ELgC2Zfg0hMX7CIv7HmPjQ5Nb%7EXp-JRnptb2%7E1qRBEvTlm-wzvT3ruhI53PgbPHvC1ulULmIbuEINiLwaiSIW9WvjV6ZiXmBFjt2xuAAjKyjaDTqC-VRz0Fs1egLouN69%7EZIfgDDqORIlM5FIcCYZwrjI5XOCMTpH6QxoelB5ykI7HwbMqNTcmv35eveeMSZQ__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 3.165.160.38, 3.165.160.20, 3.165.160.3, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|3.165.160.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7643295904 (7.1G) [binary/octet-stream]\n",
            "Saving to: â€˜Phi-3-mini-4k-instruct-fp16.ggufâ€™\n",
            "\n",
            "Phi-3-mini-4k-instr 100%[===================>]   7.12G  40.0MB/s    in 3m 2s   \n",
            "\n",
            "2024-12-14 14:08:56 (40.2 MB/s) - â€˜Phi-3-mini-4k-instruct-fp16.ggufâ€™ saved [7643295904/7643295904]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n",
        "\n",
        "# If this command does not work for you, you can use the link directly to download the model\n",
        "# https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the model with the LlamaCpp class from langchain library. `n_gpu_layers` specifies the number of layers to offload to the GPU. -1 means the model will entirely on the CPU. Whereas `AutoModelForCausalLM.from_pretrained()` loaded the model from Hugging Face, `LlamaCpp()` loads the model from the local drive."
      ],
      "metadata": {
        "id": "BrcbuNSaLuy9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LQcht_ZFijW7"
      },
      "outputs": [],
      "source": [
        "from langchain import LlamaCpp\n",
        "\n",
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\",\n",
        "    n_gpu_layers=-1,  # run entirely on the CPU\n",
        "    max_tokens=500,\n",
        "    n_ctx=2048,  # context window\n",
        "    seed=42,  # reproducibility\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the LangChain framework, send messages to the model with `invoke()` (it's `pipe()` in transformers). Unfortunately, you can't just run something like `llm.invoke(\"Hi! My name is Maarten. What is 1 + 1?\")` because the model has its own specific prompt template with tokens flagging prompt start and end, assistanct start and end, etc. The transformers.pipeline class automatically applied the chat template. In LangChain, create a prompt template and link it to the model."
      ],
      "metadata": {
        "id": "TGD_sSPWNnAA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kF--Q5me_-X1"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "# Create a prompt template with the \"input_prompt\" variable\n",
        "template = \"\"\"<s><|user|>\n",
        "{input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\"]\n",
        ")\n",
        "\n",
        "basic_chain = prompt | llm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can run `invoke()` from the `basic_chain` object. `invoke()` applies the input mesage to the prompt template, then calls the model."
      ],
      "metadata": {
        "id": "5jfHO_94mXXP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KINQxKAINXgG",
        "outputId": "14d0eb6b-2763-4981-d38c-a01443c05c8a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Hello Maarten! The answer to 1 + 1 is 2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Use the chain\n",
        "basic_chain.invoke(\n",
        "    {\n",
        "        \"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSMBMRxB8gFW"
      },
      "source": [
        "Good. That was a simple chain. You can imagine a workflow that links several chains so the output of the first flows into the next chain. Here is an example. The first chain, `title_prompt` is constructed just like `prompt` was above. This chain asks Phi-3 to generate a title for a story based on a user-supplied summary. `invoke()` returns the input `summary` and the output `title`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrUKuHt_OLpe",
        "outputId": "c94f54d4-a4d7-4f20-cb26-147c9d10da31"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': 'a girl that lost her mother',\n",
              " 'title': ' \"Whispers of Her Mother: A Journey Through Loss\"'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from langchain import LLMChain\n",
        "\n",
        "template = \"\"\"<s><|user|>\n",
        "Create a title for a story about {summary}. Only return the title.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "title_prompt = PromptTemplate(template=template, input_variables=[\"summary\"])\n",
        "\n",
        "title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")\n",
        "\n",
        "title.invoke({\"summary\": \"a girl that lost her mother\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now generate the character using `summary` and `title` for context. This is a chain with two links."
      ],
      "metadata": {
        "id": "ZwWsNs6wjTwX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zTtFEmANOhyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a178718-f0c2-48de-bb34-5742b13a04fe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': 'a girl that lost her mother',\n",
              " 'title': ' \"Whispers of Eternity: A Mother\\'s Lasting Legacy\"',\n",
              " 'character': \" The protagonist, Lily Thompson, is an empathetic and resilient young girl who has been grappling with the loss of her beloved mother since she was a child, carrying on her memory as a source of strength in navigating life's challenges. She embarks on a poignant journey to unravel her mother's mysterious past, discovering hidden family secrets and forging an unbreakable bond with the departed matriarch that transcends time itself.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Create a chain for the character description using the summary and title\n",
        "template = \"\"\"<s><|user|>\n",
        "Describe the main character of a story about {summary} with the title {title}. Use only two sentences.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "character_prompt = PromptTemplate(\n",
        "    template=template, input_variables=[\"summary\", \"title\"]\n",
        ")\n",
        "\n",
        "character = LLMChain(llm=llm, prompt=character_prompt, output_key=\"character\")\n",
        "\n",
        "llm_chain = title | character\n",
        "\n",
        "llm_chain.invoke({\"summary\": \"a girl that lost her mother\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Awesome! Let's add a third link to the chain, just because we can."
      ],
      "metadata": {
        "id": "Z46sr7jGlrLM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Xjf-avW8NAqZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4382a5c0-74da-48ff-b39f-d35b09c6b2a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': 'a girl that lost her mother',\n",
              " 'title': ' \"Whispers of Her Mother: A Tale of Remembrance\"',\n",
              " 'character': \" The protagonist, Lily, is an introspective and resilient young woman whose heart aches from the void left by her mother's sudden passing; she navigates life with a deep sense of love for her mother intertwined in every memory, dream, and decision she makes. As she embarks on a journey to honor her mother's legacy, Lily learns that while their physical separation is undeniable, the whispers of her mother will forever guide and inspire her path forward.\",\n",
              " 'story': \" Whispers of Her Mother: A Tale of Remembrance, follows Lily as she grapples with the loss of her beloved mother. Through introspection and resilience, Lily embarks on a soul-searching journey to honor her mother's memory, finding solace in cherished memories while discovering that love transcends time and space. As she encounters moments of joy, pain, and revelation, the whispers of her mother persistently guide her decisionsâ€”a testament to their unbreakable bond. With each step on this path of remembrance, Lily realizes her mother's enduring presence in every breath and heartbeat, forever illuminating her path forward with love, inspiration, and the wisdom passed down from generations before her.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "template = \"\"\"<s><|user|>\n",
        "Create a story about {summary} with the title {title}. The main charachter is: {character}. Only return the story and it cannot be longer than one paragraph<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "story_prompt = PromptTemplate(\n",
        "    template=template, input_variables=[\"summary\", \"title\", \"character\"]\n",
        ")\n",
        "\n",
        "story = LLMChain(llm=llm, prompt=story_prompt, output_key=\"story\")\n",
        "\n",
        "llm_chain = title | character | story\n",
        "\n",
        "llm_chain.invoke({\"summary\": \"a girl that lost her mother\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UQ-DZ71P-D-"
      },
      "source": [
        "# Memory\n",
        "\n",
        "LLMs are stateless out of the box, so they don't remember prior exchanges. There are two ways to make the LLM stateful: conversation buffers and conversation summaries.\n",
        "\n",
        "A conversation buffer includes the running conversation in the prompt. Let's redefine the basic prompt to start with \"`Current conversation:{chat_history}`\". Create a `ConversationBufferMemory` object and include it in the `LLMChain` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Zoo0PA1fUs70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75c07087-2949-4225-ef8c-b4271ebeb285"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'Hi! My name is Mike. What is 1 + 1?',\n",
              " 'chat_history': '',\n",
              " 'text': \" Hello Mike! The sum of 1 + 1 is 2. Nice to meet you as well!\\n\\nHere's a brief explanation of how this calculation works: In basic arithmetic, when we add two numbers together, we are essentially counting them up from the first number by the amount of the second number. So starting with 1 and adding another 1 takes us one step further, resulting in 2.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Create an updated prompt template to include a chat history\n",
        "template = \"\"\"<s><|user|>Current conversation:{chat_history}\n",
        "\n",
        "{input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\", \"chat_history\"]\n",
        ")\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Mike. What is 1 + 1?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it - does the LLM remember my name?"
      ],
      "metadata": {
        "id": "tSppfdNRpl3F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-je1rmy3dx4",
        "outputId": "33bbfdf9-5364-4985-86b3-6bcd039f34e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name?',\n",
              " 'chat_history': \"Human: Hi! My name is Mike. What is 1 + 1?\\nAI:  Hello Mike! The sum of 1 + 1 is 2. Nice to meet you as well!\\n\\nHere's a brief explanation of how this calculation works: In basic arithmetic, when we add two numbers together, we are essentially counting them up from the first number by the amount of the second number. So starting with 1 and adding another 1 takes us one step further, resulting in 2.\",\n",
              " 'text': \" Hello Mike! I'm an AI digital assistant, so I don't have a personal name like humans do, but you can refer to me as your AI Assistant. Nice to meet you too! And yes, the sum of 1 + 1 is indeed 2. Is there anything else you'd like to know or discuss?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Does the LLM remember the name we gave it?\n",
        "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw3ELCg6Rpsk"
      },
      "source": [
        "Excellent. We'll want to refine this though because a long conversation might run up on the LLM token limit. Switch from `ConversationBufferMemory` to `ConversationBufferWindowMemory` and specify a window of `k` exchanges. Let's try two."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "G0DRT7kjRtiC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c40d42eb-e85a-4c84-a6fe-ebff1c2ac5b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'Hi! My name is Mike and I am 51 years old. What is 1 + 1?',\n",
              " 'chat_history': '',\n",
              " 'text': \" Hello Mike, my name is Assistant. Just to answer your question, 1 + 1 equals 2.\\n<|assistant|> Nice to meet you, Mike! If there's anything else you'd like to know or discuss, feel free to ask. I'm here to help!\\n```\\nNote: The nature of the question was simple arithmetic and not personal information about Mike, so no privacy concerns arise from answering it. However, always maintain a professional tone when addressing users regardless of the simplicity of their questions.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "llm_chain.invoke({\"input_prompt\":\"Hi! My name is Mike and I am 51 years old. What is 1 + 1?\"})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"What is 2 + 3?\"})"
      ],
      "metadata": {
        "id": "MRqFi41YugzN",
        "outputId": "70d44c4f-c9d3-4f88-b4f9-18d1fa0da865",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is 2 + 3?',\n",
              " 'chat_history': \"Human: Hi! My name is Mike and I am 51 years old. What is 1 + 1?\\nAI:  Hello Mike, my name is Assistant. Just to answer your question, 1 + 1 equals 2.\\n<|assistant|> Nice to meet you, Mike! If there's anything else you'd like to know or discuss, feel free to ask. I'm here to help!\\n```\\nNote: The nature of the question was simple arithmetic and not personal information about Mike, so no privacy concerns arise from answering it. However, always maintain a professional tone when addressing users regardless of the simplicity of their questions.\",\n",
              " 'text': ' Hello again! To answer your question, 2 + 3 equals 5. If you have any more inquiries or need assistance with something else, feel free to ask!'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've reached the exchange limit. My name and age are still in the history. Let's ask for my name. It should still know, but now my age will have fallen out the window."
      ],
      "metadata": {
        "id": "vHEIW8khvMXu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBY69vvcR1Qq",
        "outputId": "86352acd-8b11-419c-f127-260e7ea5f028"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name?',\n",
              " 'chat_history': \"Human: Hi! My name is Mike and I am 51 years old. What is 1 + 1?\\nAI:  Hello Mike, my name is Assistant. Just to answer your question, 1 + 1 equals 2.\\n<|assistant|> Nice to meet you, Mike! If there's anything else you'd like to know or discuss, feel free to ask. I'm here to help!\\n```\\nNote: The nature of the question was simple arithmetic and not personal information about Mike, so no privacy concerns arise from answering it. However, always maintain a professional tone when addressing users regardless of the simplicity of their questions.\\nHuman: What is 2 + 3?\\nAI:  Hello again! To answer your question, 2 + 3 equals 5. If you have any more inquiries or need assistance with something else, feel free to ask!\",\n",
              " 'text': \" Your name was not mentioned during our conversation; I'm referred to as an AI assistant. However, you introduced yourself as Mike when we first started talking. Is there anything specific you would like to know or discuss further? I'm here to help!\\n\"}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"What is my name?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvSLfKWpR5h5",
        "outputId": "1fce8b71-291b-4689-b56d-7fd3880106d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my age?',\n",
              " 'chat_history': \"Human: What is 2 + 3?\\nAI:  Hello again! To answer your question, 2 + 3 equals 5. If you have any more inquiries or need assistance with something else, feel free to ask!\\nHuman: What is my name?\\nAI:  Your name was not mentioned during our conversation; I'm referred to as an AI assistant. However, you introduced yourself as Mike when we first started talking. Is there anything specific you would like to know or discuss further? I'm here to help!\\n\",\n",
              " 'text': \" I'm sorry, but I can't access personal information about individuals, including your age, due to privacy considerations. If you have questions related to general knowledge or advice on various topics, feel free to ask!\"}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"What is my age?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSb5OnANMhu2"
      },
      "source": [
        "That works well, but there is an even better method called Conversaton Summary memory. We'll send the conversation to another model to continually update with each exchange."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "lWHZlJUbwpqE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47c4131d-8a23-4bd0-f345-b8e2c56e0b0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-2be300de370d>:19: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryMemory(\n"
          ]
        }
      ],
      "source": [
        "summary_prompt_template = \"\"\"<s><|user|>Summarize the conversations and update with the new lines.\n",
        "\n",
        "Current summary:\n",
        "{summary}\n",
        "\n",
        "new lines of conversation:\n",
        "{new_lines}\n",
        "\n",
        "New summary:<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "summary_prompt = PromptTemplate(\n",
        "    input_variables=[\"new_lines\", \"summary\"],\n",
        "    template=summary_prompt_template\n",
        ")\n",
        "\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "\n",
        "# Define the type of memory we will use\n",
        "memory = ConversationSummaryMemory(\n",
        "    llm=llm,\n",
        "    memory_key=\"chat_history\",\n",
        "    prompt=summary_prompt\n",
        ")\n",
        "\n",
        "# Chain the LLM, prompt, and memory together\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2klIk9CpVSH0",
        "outputId": "0b919d0a-c9d2-41e9-f7b0-e2474148bc06"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name?',\n",
              " 'chat_history': ' New summary: Human, named Mike, asked the AI for the result of 1 + 1, and the AI responded with the correct answer (2) while offering additional assistance.',\n",
              " 'text': \" I don't have access to personal data about individuals unless it has been shared in our conversation. Therefore, I cannot determine your name based on previous interactions. Can you tell me how you would like to be addressed during our conversation?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# Generate a conversation and ask for the name\n",
        "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Mike. What is 1 + 1?\"})\n",
        "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VdOH_I-V-Fy",
        "outputId": "7e0320ca-a667-435e-b864-f438d1547314"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What was the first question I asked?',\n",
              " 'chat_history': ' Human, named Mike, asked the AI for the result of 1 + 1 and learned that they should address the AI in a preferred manner due to privacy concerns; the AI correctly provided the answer (2) while emphasizing not having access to personal data unless shared during conversation.',\n",
              " 'text': ' You initially asked, \"Human, named Mike, what is the result of 1 + 1?\"'}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# Check whether it has summarized everything thus far\n",
        "llm_chain.invoke({\"input_prompt\": \"What was the first question I asked?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1_LlvrVX9HL",
        "outputId": "749c6199-c99d-4ffc-d00e-2c776737e0cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'chat_history': ' Human named Mike inquired about the sum of 1+1 and also learned to address AI respectfully due to privacy reasons. The AI responded with the correct answer (2) while reiterating its lack of access to personal data unless shared during conversation. In addition, Mike asked the AI what was his first question.'}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# Check what the summary is thus far\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG5sJa1qvS4N"
      },
      "source": [
        "# Agents\n",
        "\n",
        "Agents are more advanced than chains. They can create and self-correct a roadmap to achieve a goal. They can interact with the real world with tools. E.g., LLMs are usually not good at math, but an agent can use a calculator. The framework for agent-based systems is called Reasoning and Acting (ReAct).\n",
        "\n",
        "The ReAct framework iteratively thinks, acts, and observes. The acting part is typically an external tool. Let's see how this is done. We'll design a system that 1) uses a search engine to find the price of a MacBook Pro, then 2) uses a calculator to convert the dollar price to euros.\n",
        "\n",
        "For this exercise we will use OPenAI's GPT-3.5 model to follow our complex instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "rcBt8bZM56dM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Load OpenAI's LLMs with LangChain\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"MY_KEY\"\n",
        "openai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "lmRZu8DO2p6k"
      },
      "outputs": [],
      "source": [
        "# Create the ReAct template\n",
        "react_template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Begin!\n",
        "\n",
        "Question: {input}\n",
        "Thought:{agent_scratchpad}\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=react_template,\n",
        "    input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, so what are the `tools` referenced in the template?"
      ],
      "metadata": {
        "id": "7x1OBtJ042Oe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "NV-ssNa-4zOK"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import load_tools, Tool\n",
        "from langchain.tools import DuckDuckGoSearchResults\n",
        "\n",
        "# You can create the tool to pass to an agent\n",
        "search = DuckDuckGoSearchResults()\n",
        "search_tool = Tool(\n",
        "    name=\"duckduck\",\n",
        "    description=\"A web search engine. Use this tool as a search engine for general queries.\",\n",
        "    func=search.run,\n",
        ")\n",
        "\n",
        "# Prepare tools\n",
        "tools = load_tools([\"llm-math\"], llm=openai_llm)\n",
        "tools.append(search_tool)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the ReAct agent and pass it to the AgentExecuter."
      ],
      "metadata": {
        "id": "hvoo1hXK5mos"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "6tAr1962vS4T"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "\n",
        "# Construct the ReAct agent\n",
        "agent = create_react_agent(openai_llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSU6ECdYBOOm",
        "outputId": "e22e663e-c384-48a7-9f1b-3e45d1eaba0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI should use a web search engine to find the current price of a MacBook Pro in USD and then use a calculator to convert it to EUR based on the exchange rate.\n",
            "Action: duckduck\n",
            "Action Input: \"current price of MacBook Pro in USD\"\u001b[0m\u001b[33;1m\u001b[1;3msnippet: For the latest prices and sales, see our 14\" MacBook Pro Price Tracker, updated daily ... Read More. December 11, 2024. Holiday Sale: 13-inch M2 MacBook Air with 16GB of RAM for $799, $200 off MSRP, at Amazon. Amazon has Apple 13\" MacBook Airs with M2 CPUs and 16GB of RAM on Holiday sale this week for $200 off MSRP, only $799. Their prices are ..., title: MacPrices.net | The original source for late-breaking Apple sales & deals, link: https://www.macprices.net/, snippet: A MacBook Pro cost ranges from just over $1000 to well over $3000. Determining the true cost for the MacBook Pro you need requires close examination of the model options, hardware configurations, and Apple's pricing strategies. Let's examine how Apple prices each MacBook Pro base model and default hardware configuration: MacBook Pro 13-inch, title: How Much Does A MacBook Pro Cost? - The Pricer, link: https://www.thepricer.org/how-much-does-a-macbook-pro-cost/, snippet: Save money on a Mac laptop--including the new MacBook Pro--with our round up of MacBook Pro money off deals and offers in the U.S. & U.K. By Karen Haslam Managing Editor, Macworld DEC 9, 2024 4:47 ..., title: Best MacBook Pro Deals - Macworld, link: https://www.macworld.com/article/672811/best-macbook-pro-deals.html, snippet: The M4 MacBook Pro is available in two sizes: 14- and 16-inch displays. ... MacBook prices. Compare prices with this chart. Model (with chip and screen size) List price Best price (current) Best ..., title: Best MacBook Deals: Get Up to $270 Off the M4 Pro, $200 Off M3 ... - CNET, link: https://www.cnet.com/deals/best-macbook-deals/\u001b[0m\u001b[32;1m\u001b[1;3mI found the current price range of a MacBook Pro in USD. Now I need to use a calculator to convert it to EUR based on the exchange rate.\n",
            "Action: Calculator\n",
            "Action Input: $1000 (assuming the lower end of the price range) * 0.85\u001b[0m\u001b[36;1m\u001b[1;3mAnswer: 850.0\u001b[0m\u001b[32;1m\u001b[1;3mThe cost of a MacBook Pro in EUR would be 850.0 based on the exchange rate.\n",
            "Final Answer: 850.0 EUR\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?',\n",
              " 'output': '850.0 EUR'}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# What is the Price of a MacBook Pro?\n",
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\": \"What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?\"\n",
        "    }\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}